# Chapter 08 â€” The Transformer Block

**Summary:** Wiring attention, normalization, and feed-forward into one unit.

## Objectives
- Compose attention, residual connections, layer norm, and MLP.
- Implement a reusable block module.

## Notes
_TBD as the chapter is written._

## Checkpoint
- Suggested tag: `chapter-08-block`.
